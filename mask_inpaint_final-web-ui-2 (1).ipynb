{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714796451599,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"KWRQxZ9lVTCi","outputId":"807901ac-2196-4985-bd38-7bff4f67ec84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["%%writefile requirements.txt\n","mtcnn\n","ultralytics\n","dlib\n","dotmap==1.3.14\n","face-recognition\n","face-recognition-models\n","numpy\n","opencv-python\n","Pillow\n","tqdm\n","wincertstore==0.2\n","imutils\n","requests==2.24.0"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95940,"status":"ok","timestamp":1714796551814,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"JF6PHdLuVU-z","outputId":"523fc627-12de-4b3b-fd85-c1ab2c286e51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mtcnn (from -r requirements.txt (line 1))\n","  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ultralytics (from -r requirements.txt (line 2))\n","  Downloading ultralytics-8.2.8-py3-none-any.whl (755 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.2/755.2 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (19.24.4)\n","Collecting dotmap==1.3.14 (from -r requirements.txt (line 4))\n","  Downloading dotmap-1.3.14-py3-none-any.whl (10 kB)\n","Collecting face-recognition (from -r requirements.txt (line 5))\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Collecting face-recognition-models (from -r requirements.txt (line 6))\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.25.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.8.0.76)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (9.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.2)\n","Collecting wincertstore==0.2 (from -r requirements.txt (line 11))\n","  Downloading wincertstore-0.2-py2.py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (0.5.4)\n","Collecting requests==2.24.0 (from -r requirements.txt (line 13))\n","  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chardet<4,>=3.0.2 (from requests==2.24.0->-r requirements.txt (line 13))\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna<3,>=2.5 (from requests==2.24.0->-r requirements.txt (line 13))\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests==2.24.0->-r requirements.txt (line 13))\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.24.0->-r requirements.txt (line 13)) (2024.2.2)\n","Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn->-r requirements.txt (line 1)) (2.15.0)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (6.0.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (2.2.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (0.17.1+cu121)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (9.0.0)\n","Collecting thop>=0.1.1 (from ultralytics->-r requirements.txt (line 2))\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics->-r requirements.txt (line 2)) (0.13.1)\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face-recognition->-r requirements.txt (line 5)) (8.1.7)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics->-r requirements.txt (line 2)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics->-r requirements.txt (line 2)) (2024.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics->-r requirements.txt (line 2))\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics->-r requirements.txt (line 2)) (1.3.0)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566170 sha256=e89418b5695fcc41f1a55503d9e3986af0f4e20d1cc51e3e216aa691f9c7720b\n","  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n","Successfully built face-recognition-models\n","Installing collected packages: wincertstore, face-recognition-models, dotmap, chardet, urllib3, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, idna, face-recognition, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mtcnn, nvidia-cusolver-cu12, thop, ultralytics\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.7\n","    Uninstalling idna-3.7:\n","      Successfully uninstalled idna-3.7\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 1.4.0 requires requests>=2.27.1, but you have requests 2.24.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.24.0 which is incompatible.\n","tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.24.0 which is incompatible.\n","yfinance 0.2.38 requires requests>=2.31, but you have requests 2.24.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed chardet-3.0.4 dotmap-1.3.14 face-recognition-1.3.0 face-recognition-models-0.3.0 idna-2.10 mtcnn-0.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 requests-2.24.0 thop-0.1.1.post2209072238 ultralytics-8.2.8 urllib3-1.25.11 wincertstore-0.2\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52288,"status":"ok","timestamp":1714796614397,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"Y4OcKMEeflFq","outputId":"33ee03ea-4808-48ee-8150-e88f5fffc29d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.24.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1qn1fKj-4iwykSZl_GT9kjz2UTnbMlU36\n","From (redirected): https://drive.google.com/uc?id=1qn1fKj-4iwykSZl_GT9kjz2UTnbMlU36&confirm=t&uuid=f20a3d90-100c-45fd-8e8d-1ced26260113\n","To: /content/model.pth\n","100% 1.64G/1.64G [00:28<00:00, 58.4MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1eJysHy7XVw19KrDInZ5wylXChxBrPwGb\n","To: /content/best.pt\n","100% 6.24M/6.24M [00:00<00:00, 174MB/s]\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1VpD27jHOPaOJRKFqOO_txLHAE05CbtLU\n","From (redirected): https://drive.google.com/uc?id=1VpD27jHOPaOJRKFqOO_txLHAE05CbtLU&confirm=t&uuid=760d624d-33da-4873-b322-96bebcded3f5\n","To: /content/BEST_checkpoint_r101.pth\n","100% 209M/209M [00:01<00:00, 136MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1RmJpTdqx28O__4r5nTebfm10Rk8ZRQok\n","To: /content/men.jpeg\n","100% 10.7k/10.7k [00:00<00:00, 48.9MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1gtaEf5gwBX3renRDJWpMEycyS41TlAyc\n","To: /content/women.jpeg\n","100% 10.8k/10.8k [00:00<00:00, 42.5MB/s]\n"]}],"source":["!pip install --upgrade --no-cache-dir gdown\n","\n","#https://drive.google.com/file/d/1dIvKsW36j2D7AN2SBh-ZinF9X9iZoCon/view?usp=sharing\n","#!gdown https://drive.google.com/uc?id=1dIvKsW36j2D7AN2SBh-ZinF9X9iZoCon\n","\n","#https://drive.google.com/file/d/1qn1fKj-4iwykSZl_GT9kjz2UTnbMlU36/view?usp=sharing\n","!gdown https://drive.google.com/uc?id=1qn1fKj-4iwykSZl_GT9kjz2UTnbMlU36\n","\n","#https://drive.google.com/file/d/1eJysHy7XVw19KrDInZ5wylXChxBrPwGb/view?usp=sharing\n","!gdown https://drive.google.com/uc?id=1eJysHy7XVw19KrDInZ5wylXChxBrPwGb\n","\n","#https://drive.google.com/file/d/1VpD27jHOPaOJRKFqOO_txLHAE05CbtLU/view?usp=sharing\n","!gdown https://drive.google.com/uc?id=1VpD27jHOPaOJRKFqOO_txLHAE05CbtLU\n","\n","#https://drive.google.com/file/d/1RmJpTdqx28O__4r5nTebfm10Rk8ZRQok/view?usp=sharing\n","!gdown https://drive.google.com/uc?id=1RmJpTdqx28O__4r5nTebfm10Rk8ZRQok\n","\n","#https://drive.google.com/file/d/1gtaEf5gwBX3renRDJWpMEycyS41TlAyc/view?usp=sharing\n","!gdown https://drive.google.com/uc?id=1gtaEf5gwBX3renRDJWpMEycyS41TlAyc"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2966,"status":"ok","timestamp":1714796651357,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"4hhZWpfjgClX","outputId":"d565e61a-b2de-434e-c781-bdebdec7f730"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'RefFaceInpainting'...\n","remote: Enumerating objects: 110, done.\u001b[K\n","remote: Counting objects: 100% (35/35), done.\u001b[K\n","remote: Compressing objects: 100% (35/35), done.\u001b[K\n","remote: Total 110 (delta 17), reused 0 (delta 0), pack-reused 75\u001b[K\n","Receiving objects: 100% (110/110), 4.65 MiB | 14.92 MiB/s, done.\n","Resolving deltas: 100% (31/31), done.\n"]}],"source":["!git clone https://github.com/WuyangLuo/RefFaceInpainting.git\n","\n","!mkdir -p /content/RefFaceInpainting/pretrained_models\n","!mv '/content/BEST_checkpoint_r101.pth' /content/RefFaceInpainting/pretrained_models/."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":828,"status":"ok","timestamp":1714796657103,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"CZq2xbssT3m5","outputId":"de9506ce-a8ea-4bcf-862e-f742b5101372"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/RefFaceInpainting/configs/config.yaml\n"]}],"source":["%%writefile /content/RefFaceInpainting/configs/config.yaml\n","# basic setting\n","is_train: True\n","worker: 8\n","mask_shape: [128, 128]\n","#GPU_NUM: 1\n","EYE_shape: [55, 60]\n","NOSE_shape: [50, 60]\n","MOUTH_shape: [44, 90]\n","\n","# train\n","# basic\n","batch_size: 25\n","shuffle: true\n","max_epoch: 500\n","epoch_start: 1      # the starting epoch count\n","\n","# optimizer\n","beta1: 0.5\n","beta2: 0.999\n","weight_decay: 0.0001\n","# lr\n","lr: 0.0002\n","lr_policy: 'linear'\n","# linear\n","epoch_init_lr: 500  # of epochs at starting learning rate'\n","niter_decay: 1500    # of epochs to linearly decay learning rate to zero\n","# step\n","step_size: 50\n","gamma: 0.5\n","\n","# print\n","visual_img_freq: 5000\n","print_loss_freq: 5000\n","save_epoch_freq: 20\n","\n","# test\n","test_batch_size: 1   # must be one\n","results_root: 'results/'\n","test_freq: 10\n","dims: 2048\n","\n","# dataset\n","dataset_dir: ''  # path to dataset\n","input_nc: 3\n","mask_nc: 1\n","output_nc: 3\n","crop_size: 256\n","crop: True\n","flip: True\n","\n","# net\n","use_dropout: False\n","\n","# generator\n","ngf: 32\n","G_norm_type: in\n","downs_num: 4\n","num_block: 6\n","\n","style_dim: 64\n","\n","# discriminator\n","ndf: 64\n","n_layers_D: 3\n","D_norm_type: batch\n","gan_mode: hinge\n","num_D: 3\n","# compD\n","ndf_comp: 32\n","D_comp_norm_type: batch\n","\n","# loss\n","# stage1\n","no_ganFeat_loss: False\n","no_vgg_loss: False\n","lambda_L1: 1\n","lambda_feat: 10\n","lambda_vgg: 10\n","lambda_arcface: 3\n","lambda_comp: 0.5"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":788,"status":"ok","timestamp":1714796665467,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"tCC1z-Kky_rE","outputId":"7ffd55d4-ae40-44c0-d21e-121cc0b210e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/RefFaceInpainting/utils.py\n"]}],"source":["%%writefile /content/RefFaceInpainting/utils.py\n","from torch.optim import lr_scheduler\n","import torch.nn as nn\n","import os\n","import math\n","import yaml\n","import torch.nn.init as init\n","import torch\n","import functools\n","import cv2\n","import numpy as np\n","from PIL import Image\n","\n","def get_config(config):\n","    with open(config, 'r') as stream:\n","        return yaml.load(stream,Loader=yaml.Loader)\n","\n","def prepare_sub_folder(output_directory):\n","    image_directory = os.path.join(output_directory, 'images')\n","    if not os.path.exists(image_directory):\n","        print(\"Creating directory: {}\".format(image_directory))\n","        os.makedirs(image_directory)\n","    checkpoint_directory = os.path.join(output_directory, 'checkpoints')\n","    if not os.path.exists(checkpoint_directory):\n","        print(\"Creating directory: {}\".format(checkpoint_directory))\n","        os.makedirs(checkpoint_directory)\n","    results_directory = os.path.join(output_directory, 'results')\n","    if not os.path.exists(results_directory):\n","        print(\"Creating directory: {}\".format(results_directory))\n","        os.makedirs(results_directory)\n","    return checkpoint_directory, image_directory, results_directory\n","\n","# dataset\n","IMG_EXTENSIONS = [\n","    '.jpg', '.JPG', '.jpeg', '.JPEG',\n","    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n","]\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","def make_dataset(dir, max_dataset_size=float(\"inf\")):\n","    images = []\n","    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n","\n","    for root, _, fnames in sorted(os.walk(dir)):\n","        for fname in fnames:\n","            if is_image_file(fname):\n","                path = os.path.join(root, fname)\n","                images.append(path)\n","    return images[:min(max_dataset_size, len(images))]\n","\n","\n","def get_scheduler(optimizer, cfg):\n","    \"\"\"Return a learning rate scheduler\n","\n","    Parameters:\n","        optimizer          -- the optimizer of the network\n","        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．\n","                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n","\n","    For 'linear', we keep the same learning rate for the first <opt.niter> epochs\n","    and linearly decay the rate to zero over the next <opt.niter_decay> epochs.\n","    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n","    See https://pytorch.org/docs/stable/optim.html for more details.\n","    \"\"\"\n","    if cfg['lr_policy'] == 'linear':\n","        def lambda_rule(epoch):\n","            lr_l = 1.0 - max(0, epoch + cfg['epoch_start'] - cfg['epoch_init_lr']) / float(cfg['niter_decay'] + 1)\n","            return lr_l\n","        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n","    elif cfg['lr_policy'] == 'step':\n","        scheduler = lr_scheduler.StepLR(optimizer, step_size=cfg['step_size'], gamma=0.1)\n","    elif cfg['lr_policy'] == 'plateau':\n","        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n","    elif cfg['lr_policy'] == 'cosine':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['niter_decay'], eta_min=0)\n","    else:\n","        return NotImplementedError('learning rate policy [%s] is not implemented', cfg['lr_policy'])\n","    return scheduler\n","\n","\n","def weights_init(init_type='gaussian'):\n","    def init_fun(m):\n","        classname = m.__class__.__name__\n","        if (classname.find('Conv') == 0 or classname.find('Linear') == 0) and hasattr(m, 'weight'):\n","            # print m.__class__.__name__\n","            if init_type == 'gaussian':\n","                init.normal_(m.weight.data, 0.0, 0.02)\n","            elif init_type == 'xavier':\n","                init.xavier_normal_(m.weight.data, gain=math.sqrt(2))\n","            elif init_type == 'kaiming':\n","                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","            elif init_type == 'orthogonal':\n","                init.orthogonal_(m.weight.data, gain=math.sqrt(2))\n","            elif init_type == 'default':\n","                pass\n","            else:\n","                assert 0, \"Unsupported initialization: {}\".format(init_type)\n","            if hasattr(m, 'bias') and m.bias is not None:\n","                init.constant_(m.bias.data, 0.0)\n","\n","    return init_fun\n","\n","\n","def save_network(net, label, epoch, cfg):\n","    save_filename = '%03d _%s.pth' % (epoch, label)\n","    save_path = os.path.join(cfg['checkpoints_dir'], save_filename)\n","    torch.save(net.cpu().state_dict(), save_path)\n","    if torch.cuda.is_available():\n","        net.cuda()\n","\n","def save_latest_network(net, epoch, label, cfg):\n","    save_filename = 'latest_%s.pth' % (label)\n","    save_path = os.path.join(cfg['checkpoints_dir'], save_filename)\n","    save_file = {'epoch': epoch, 'net': net.cpu().state_dict()}\n","    torch.save(save_file, save_path)\n","    if torch.cuda.is_available():\n","        net.cuda()\n","\n","# Get model list for resume\n","def get_model_list(dirname, key):\n","    if os.path.exists(dirname) is False:\n","        return None\n","    gen_models = [os.path.join(dirname, f) for f in os.listdir(dirname) if\n","                  os.path.isfile(os.path.join(dirname, f)) and key in f and \".pt\" in f]\n","    if gen_models is None:\n","        return None\n","    gen_models.sort()\n","    last_model_name = gen_models[0]\n","    return last_model_name\n","\n","\n","def get_norm_layer(norm_type='instance'):\n","    \"\"\"Return a normalization layer\n","\n","    Parameters:\n","        norm_type (str) -- the name of the normalization layer: batch | instance | none\n","\n","    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n","    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n","    \"\"\"\n","    if norm_type == 'batch':\n","        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n","    elif norm_type == 'instance':\n","        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n","    elif norm_type == 'none':\n","        norm_layer = None\n","    else:\n","        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n","    return norm_layer\n","\n","\n","def generate_draw(mask, maks_name=None):\n","    min_num_vertex = 4\n","    max_num_vertex = 10\n","    min_width = 4\n","    max_width = 6\n","    mean_angle = 2 * math.pi / 4\n","    angle_range = 2 * math.pi / 15\n","\n","    if 'lip'in maks_name:\n","        mean_angle = 0\n","        angle_range = 2 * math.pi / 15\n","        min_width = 2\n","        max_width = 3\n","\n","    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (max_width, max_width))\n","    mask = cv2.erode(mask, kernel)\n","    fg = np.array(np.greater(mask, 0.5).astype(np.uint8))\n","    num_fg = np.sum(fg)\n","\n","    H, W = mask.shape[0], mask.shape[1]\n","    average_radius = math.sqrt(H * H + W * W) / 8\n","    mask = np.zeros((W, H))\n","\n","    pointY, pointX = np.where(fg > 0.5)\n","\n","    th_olrate = np.random.uniform(0.3, 0.7)\n","    olrate = 0.0\n","    while olrate <= th_olrate:\n","        num_vertex = np.random.randint(min_num_vertex, max_num_vertex)\n","        angle_min = mean_angle - np.random.uniform(0, angle_range)\n","        angle_max = mean_angle + np.random.uniform(0, angle_range)\n","        vertex = []\n","\n","        if len(pointX) == 0:\n","            break\n","\n","        startidx = np.random.randint(len(pointX))\n","\n","        vertex.append((pointX[startidx], pointY[startidx]))  # set start point\n","        num = 0\n","        while num < num_vertex:\n","            if num % 2 == 0:\n","                angle = 2 * math.pi - np.random.uniform(angle_min, angle_max)\n","            else:\n","                angle = np.random.uniform(angle_min, angle_max)\n","            r = np.clip(np.random.normal(loc=average_radius, scale=average_radius // 2), 0, 2 * average_radius)\n","            new_x = int(vertex[-1][0] + r * math.cos(angle))\n","            new_y = int(vertex[-1][1] + r * math.sin(angle))\n","\n","            if (new_x in range(H)) and (new_y in range(W)) and (fg[new_y, new_x] == 1):\n","                vertex.append((new_x, new_y))\n","                num += 1\n","            else:\n","                pass\n","\n","        width = int(np.random.uniform(min_width, max_width))\n","        # draw lines\n","        for i in range(len(vertex)-1):\n","            cv2.line(mask, vertex[i], vertex[i+1], 255, width)\n","\n","        cur_mask_fg = np.array(np.greater(mask, 0.5).astype(np.uint8)) * fg\n","        num_cur_mask = np.sum(cur_mask_fg)\n","        olrate = num_cur_mask / num_fg\n","\n","    out = np.array(np.greater(mask*fg, 127).astype(np.uint8))\n","\n","    return out\n","\n","\n","def tensor2im(input_image, imtype=np.uint8, no_fg=True):\n","    \"\"\"\"Converts a Tensor array into a numpy image array.\n","\n","    Parameters:\n","        input_image (tensor) --  the input image tensor array\n","        imtype (type)        --  the desired type of the converted numpy array\n","        no_fg: binary image and don't transform\n","    \"\"\"\n","    if not isinstance(input_image, np.ndarray):\n","        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n","            image_tensor = input_image.data\n","        else:\n","            return input_image\n","        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array, only take the first output\n","        if no_fg:\n","            image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n","        else:\n","            image_numpy = (np.transpose(image_numpy, (1, 2, 0))) * 255.0\n","        image_numpy = np.clip(image_numpy, 0, 255)\n","    else:  # if it is a numpy array, do nothing\n","        image_numpy = input_image\n","    return cv2.cvtColor(image_numpy.astype(imtype), cv2.COLOR_RGB2BGR)\n","\n","def lab2im(input_image, imtype=np.uint8):\n","    \"\"\"\"Converts a Tensor array into a numpy image array.\n","\n","    Parameters:\n","        input_image (tensor) --  the input image tensor array\n","        imtype (type)        --  the desired type of the converted numpy array\n","        no_fg: binary image and don't transform\n","    \"\"\"\n","    if not isinstance(input_image, np.ndarray):\n","        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n","            image_tensor = input_image.data\n","        else:\n","            return input_image\n","        image_numpy = image_tensor[0].cpu().float().numpy()  # convert it into a numpy array, only take the first output\n","        image_numpy = (np.transpose(image_numpy, (1, 2, 0)))\n","        image_numpy = np.clip(image_numpy, 0, 255)\n","    else:  # if it is a numpy array, do nothing\n","        image_numpy = input_image\n","    return image_numpy.astype(imtype)\n","\n","\n","def list_ave(list):\n","    sum = 0.\n","    for l in list:\n","        sum += l\n","    return sum/len(list)\n","\n","def normalization(data):\n","    _range = np.max(data) - np.min(data)\n","    return (data - np.min(data)) / _range\n","\n","def tensor2hm(input):\n","    if not isinstance(input, np.ndarray):\n","        if isinstance(input, torch.Tensor):  # get the data from a variable\n","            image_tensor = input.data\n","        else:\n","            return input\n","        image_numpy = image_tensor[0].cpu().float().numpy()\n","        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n","\n","        posi = np.sqrt(image_numpy*image_numpy)\n","        sum = np.sum(posi, axis=2)\n","\n","        norm = normalization(sum)\n","        out = norm * 255\n","\n","        return out.astype(np.uint8)\n","\n","def flow_to_image(flow):\n","    \"\"\"Transfer flow map to image.\n","    Part of code forked from flownet.\n","    \"\"\"\n","    out = []\n","    maxu = -999.\n","    maxv = -999.\n","    minu = 999.\n","    minv = 999.\n","    maxrad = -1\n","    for i in range(flow.shape[0]):\n","        u = flow[i, :, :, 0]\n","        v = flow[i, :, :, 1]\n","        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)\n","        u[idxunknow] = 0\n","        v[idxunknow] = 0\n","        maxu = max(maxu, np.max(u))\n","        minu = min(minu, np.min(u))\n","        maxv = max(maxv, np.max(v))\n","        minv = min(minv, np.min(v))\n","        rad = np.sqrt(u ** 2 + v ** 2)\n","        maxrad = max(maxrad, np.max(rad))\n","        u = u / (maxrad + np.finfo(float).eps)\n","        v = v / (maxrad + np.finfo(float).eps)\n","        img = compute_color(u, v)\n","        out.append(img)\n","    return np.float32(np.uint8(out))\n","\n","def compute_color(u, v):\n","    h, w = u.shape\n","    img = np.zeros([h, w, 3])\n","    nanIdx = np.isnan(u) | np.isnan(v)\n","    u[nanIdx] = 0\n","    v[nanIdx] = 0\n","    # colorwheel = COLORWHEEL\n","    colorwheel = make_color_wheel()\n","    ncols = np.size(colorwheel, 0)\n","    rad = np.sqrt(u ** 2 + v ** 2)\n","    a = np.arctan2(-v, -u) / np.pi\n","    fk = (a + 1) / 2 * (ncols - 1) + 1\n","    k0 = np.floor(fk).astype(int)\n","    k1 = k0 + 1\n","    k1[k1 == ncols + 1] = 1\n","    f = fk - k0\n","    for i in range(np.size(colorwheel, 1)):\n","        tmp = colorwheel[:, i]\n","        col0 = tmp[k0 - 1] / 255\n","        col1 = tmp[k1 - 1] / 255\n","        col = (1 - f) * col0 + f * col1\n","        idx = rad <= 1\n","        col[idx] = 1 - rad[idx] * (1 - col[idx])\n","        notidx = np.logical_not(idx)\n","        col[notidx] *= 0.75\n","        img[:, :, i] = np.uint8(np.floor(255 * col * (1 - nanIdx)))\n","    return img\n","\n","def make_color_wheel():\n","    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n","    ncols = RY + YG + GC + CB + BM + MR\n","    colorwheel = np.zeros([ncols, 3])\n","    col = 0\n","    # RY\n","    colorwheel[0:RY, 0] = 255\n","    colorwheel[0:RY, 1] = np.transpose(np.floor(255 * np.arange(0, RY) / RY))\n","    col += RY\n","    # YG\n","    colorwheel[col:col + YG, 0] = 255 - np.transpose(np.floor(255 * np.arange(0, YG) / YG))\n","    colorwheel[col:col + YG, 1] = 255\n","    col += YG\n","    # GC\n","    colorwheel[col:col + GC, 1] = 255\n","    colorwheel[col:col + GC, 2] = np.transpose(np.floor(255 * np.arange(0, GC) / GC))\n","    col += GC\n","    # CB\n","    colorwheel[col:col + CB, 1] = 255 - np.transpose(np.floor(255 * np.arange(0, CB) / CB))\n","    colorwheel[col:col + CB, 2] = 255\n","    col += CB\n","    # BM\n","    colorwheel[col:col + BM, 2] = 255\n","    colorwheel[col:col + BM, 0] = np.transpose(np.floor(255 * np.arange(0, BM) / BM))\n","    col += + BM\n","    # MR\n","    colorwheel[col:col + MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n","    colorwheel[col:col + MR, 0] = 255\n","    return colorwheel\n","\n","def extract_image_patches(images, ksizes, strides, rates, padding='same'):\n","    \"\"\"\n","    Extract patches from images and put them in the C output dimension.\n","    :param padding:\n","    :param images: [batch, channels, in_rows, in_cols]. A 4-D Tensor with shape\n","    :param ksizes: [ksize_rows, ksize_cols]. The size of the sliding window for\n","     each dimension of images\n","    :param strides: [stride_rows, stride_cols]\n","    :param rates: [dilation_rows, dilation_cols]\n","    :return: A Tensor\n","    \"\"\"\n","    assert len(images.size()) == 4\n","    assert padding in ['same', 'valid']\n","    batch_size, channel, height, width = images.size()\n","\n","    if padding == 'same':\n","        images = same_padding(images, ksizes, strides, rates)\n","    elif padding == 'valid':\n","        pass\n","    else:\n","        raise NotImplementedError('Unsupported padding type: {}.\\\n","                Only \"same\" or \"valid\" are supported.'.format(padding))\n","\n","    unfold = torch.nn.Unfold(kernel_size=ksizes,\n","                             dilation=rates,\n","                             padding=0,\n","                             stride=strides)\n","    patches = unfold(images)\n","    return patches  # [N, C*k*k, L], L is the total number of such blocks\n","\n","def reduce_mean(x, axis=None, keepdim=False):\n","    if not axis:\n","        axis = range(len(x.shape))\n","    for i in sorted(axis, reverse=True):\n","        x = torch.mean(x, dim=i, keepdim=keepdim)\n","    return x\n","\n","def reduce_sum(x, axis=None, keepdim=False):\n","    if not axis:\n","        axis = range(len(x.shape))\n","    for i in sorted(axis, reverse=True):\n","        x = torch.sum(x, dim=i, keepdim=keepdim)\n","    return x\n","\n","def same_padding(images, ksizes, strides, rates):\n","    assert len(images.size()) == 4\n","    batch_size, channel, rows, cols = images.size()\n","    out_rows = (rows + strides[0] - 1) // strides[0]\n","    out_cols = (cols + strides[1] - 1) // strides[1]\n","    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n","    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n","    padding_rows = max(0, (out_rows-1)*strides[0]+effective_k_row-rows)\n","    padding_cols = max(0, (out_cols-1)*strides[1]+effective_k_col-cols)\n","    # Pad the input\n","    padding_top = int(padding_rows / 2.)\n","    padding_left = int(padding_cols / 2.)\n","    padding_bottom = padding_rows - padding_top\n","    padding_right = padding_cols - padding_left\n","    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n","    images = torch.nn.ZeroPad2d(paddings)(images)\n","    return images\n","\n","def print_options(opts):\n","    message = ''\n","    message += '----------------- Options ---------------\\n'\n","    for k, v in sorted(vars(opts).items()):\n","        comment = ''\n","        message += '{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment)\n","    message += '----------------- End -------------------'\n","    print(message)\n","\n","def mask_patch(x, mask_rate):\n","    _, _, h, w = x.size()\n","    mask_length = int(w * mask_rate)  # masked length\n","    w1 = w - mask_length\n","    img_masked = x[:, :, :, w1:]\n","    return img_masked\n","\n","def spatial_discounting_mask(config):\n","    \"\"\"Generate spatial discounting mask constant.\n","\n","    Spatial discounting mask is first introduced in publication:\n","        Generative Image Inpainting with Contextual Attention, Yu et al.\n","\n","    Args:\n","        config: Config should have configuration including HEIGHT, WIDTH,\n","            DISCOUNTED_MASK.\n","\n","    Returns:\n","        tf.Tensor: spatial discounting mask\n","\n","    \"\"\"\n","    gamma = config['spatial_discounting_gamma']\n","    height, width = config['mask_shape']\n","    shape = [1, 1, height, width]\n","    if config['discounted_mask']:\n","        mask_values = np.ones((height, width))\n","        for i in range(height):\n","            for j in range(width):\n","                mask_values[i, j] = max(\n","                    gamma ** min(i, height - i),\n","                    gamma ** min(j, width - j))\n","        mask_values = np.expand_dims(mask_values, 0)\n","        mask_values = np.expand_dims(mask_values, 0)\n","    else:\n","        mask_values = np.ones(shape)\n","    spatial_discounting_mask_tensor = torch.tensor(mask_values, dtype=torch.float32)\n","\n","    spatial_discounting_mask_tensor = spatial_discounting_mask_tensor.cuda()\n","    return spatial_discounting_mask_tensor"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714796673275,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"vFh4vtRuySAx","outputId":"7316492b-7051-414e-b51f-a8a79566ee76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/RefFaceInpainting/test.py\n"]}],"source":["%%writefile /content/RefFaceInpainting/test.py\n","import numpy as np\n","import datetime\n","import torch\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","import os\n","import shutil\n","import cv2\n","from utils import *\n","from trainer import Trainer\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n","\n","gt_img_path = '/content/celebID/test_gt_subtest/'     # path to test_gt_subtest\n","ref_path = '/content/celebID/test_ref/'               # path to test_ref\n","lab_path = '/content/celebID/ref_labels/'             # path to ref_labels\n","\n","mask_path = '/content/celebID/test_masks/'            # path to test_masks\n","\n","save_path = 'results_multi_ref_subset/'\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","\n","img_list = os.listdir(gt_img_path)[:50]\n","img_list.sort()\n","\n","# =======================================================================================================\n","# =======================================================================================================\n","\n","def preprocess(img):\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = transforms.ToTensor()(img)\n","    img = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(img)\n","    img = img.cuda().unsqueeze(0)\n","    return img\n","\n","def get_transform_lab(lab):\n","    lab = transforms.ToTensor()(lab)\n","    lab = lab.cuda().unsqueeze(0)\n","    return lab\n","\n","def eval_model(trainer):\n","    for idx, img_name in enumerate(img_list):\n","        for idx_ref, ref_name in enumerate(img_list):\n","            # print(idx_ref, ref_name)\n","            img = cv2.imread(os.path.join(gt_img_path, img_name))\n","            img = preprocess(img)\n","            ref = cv2.imread(os.path.join(ref_path, ref_name))\n","            ref = preprocess(ref)\n","\n","            lab_ref = cv2.imread(lab_path + ref_name[:-4]+'.png', 0)\n","            lab_ref = get_transform_lab(lab_ref)\n","            lab_ref = lab_ref * 255.0\n","\n","            mask = cv2.imread(os.path.join(mask_path, '%03d'%(idx)+'.png'), 0)\n","            mask = mask.reshape((1,) + mask.shape).astype(np.float32) / 255\n","            mask = torch.from_numpy(mask).unsqueeze(0).cuda()\n","\n","            masked_img = img * (1. - mask)\n","\n","            masked_fake = trainer.test(img, masked_img, ref, mask, lab_ref)\n","\n","            size = 256\n","            n = 4\n","            m = 3\n","            img_concat = np.ones((size, size*n + m*(n-1), 3)) * 255\n","\n","            print(masked_img.size())\n","            masked_img = tensor2im(masked_img, no_fg=True)\n","            ref = tensor2im(ref, no_fg=True)\n","            gt = tensor2im(img, no_fg=True)\n","            masked_fake = tensor2im(masked_fake, no_fg=True)\n","\n","            print('{}_{}.jpg'.format(idx, idx_ref))\n","            curr_save_dir = os.path.join(save_path, str(idx))\n","            if not os.path.exists(curr_save_dir):\n","                os.makedirs(curr_save_dir)\n","            cv2.imwrite(os.path.join(curr_save_dir, 'gt_{}.jpg'.format(idx)), gt)\n","            cv2.imwrite(os.path.join(curr_save_dir, 'ref_{}.jpg'.format(idx_ref)), ref)\n","            cv2.imwrite(os.path.join(curr_save_dir, 'masked_img.jpg'), masked_img)\n","            cv2.imwrite(os.path.join(curr_save_dir, '{}.jpg'.format(idx_ref)), masked_fake)\n","\n","\n","cfg = get_config('configs/config.yaml')\n","model_path = '/content/model.pth'\n","trainer = Trainer(cfg)\n","trainer.cuda()\n","trainer.netG.load_state_dict(torch.load(model_path)['netG'])\n","trainer.set_eval()\n","eval_model(trainer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ieX65kAIyrDV"},"outputs":[],"source":["# !python -u test.py"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":698,"status":"ok","timestamp":1714796677860,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"zIdMnBBgUreT","outputId":"ab888e33-40f7-4111-c1d9-9b64d1862289"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/RefFaceInpainting\n"]}],"source":["%cd /content/RefFaceInpainting"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20042,"status":"ok","timestamp":1714796701554,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"oIhBzIMP8RAw","outputId":"52be7303-d239-453b-e5b2-140d16e74884"},"outputs":[{"output_type":"stream","name":"stdout","text":["load arcface\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:05<00:00, 100MB/s]\n"]}],"source":["import numpy as np\n","import datetime\n","import torch\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","import os\n","import shutil\n","import cv2\n","from utils import *\n","from trainer import Trainer\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n","\n","cfg = get_config('configs/config.yaml')\n","model_path = '/content/model.pth'\n","trainer = Trainer(cfg)\n","trainer.cuda()\n","trainer.netG.load_state_dict(torch.load(model_path)['netG'])\n","trainer.set_eval()\n","\n","def preprocess(img):\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = transforms.ToTensor()(img)\n","    img = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(img)\n","    img = img.cuda().unsqueeze(0)\n","    return img\n","\n","def get_transform_lab(lab):\n","    lab = transforms.ToTensor()(lab)\n","    lab = lab.cuda().unsqueeze(0)\n","    return lab\n","\n","def get_inpainted_image(input_img_path,ref_img_path, label_img_path, mask_img_path):\n","    img = cv2.imread(input_img_path)\n","    img = preprocess(img)\n","    ref = cv2.imread(ref_img_path)\n","    ref = preprocess(ref)\n","\n","    lab_ref = cv2.imread(label_img_path, 0)\n","    lab_ref = get_transform_lab(lab_ref)\n","    lab_ref = lab_ref * 255.0\n","\n","    mask = cv2.imread(mask_img_path, 0)\n","    mask = mask.reshape((1,) + mask.shape).astype(np.float32) / 255\n","    mask = torch.from_numpy(mask).unsqueeze(0).cuda()\n","\n","    masked_img = img * (1. - mask)\n","\n","    masked_fake = trainer.test(img, masked_img, ref, mask, lab_ref)\n","\n","    size = 256\n","    n = 4\n","    m = 3\n","    img_concat = np.ones((size, size*n + m*(n-1), 3)) * 255\n","\n","    # print(masked_img.size())\n","    masked_img = tensor2im(masked_img, no_fg=True)\n","    ref = tensor2im(ref, no_fg=True)\n","    gt = tensor2im(img, no_fg=True)\n","    masked_fake = tensor2im(masked_fake, no_fg=True)\n","\n","    return masked_fake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQxrxqyq9ZE4"},"outputs":[],"source":["# inpainted_mask = cv2.cvtColor(inpainted_mask, cv2.COLOR_BGR2RGB)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"EMcI0AxsHQZJ","executionInfo":{"status":"ok","timestamp":1714796714663,"user_tz":-330,"elapsed":7323,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"}}},"outputs":[],"source":["from ultralytics import YOLO\n","import cv2\n","from mtcnn import MTCNN\n","\n","model = YOLO('/content/best.pt',verbose=False)\n","detector = MTCNN()\n","\n","def predict(img, classes=[], conf=0.5):\n","    if classes:\n","        results = model.predict(img, classes=classes, conf=conf)\n","    else:\n","        results = model.predict(img, conf=conf)\n","\n","    return results\n","\n","def convert_coordinates(x1, y1, x2, y2):\n","    x = min(x1, x2)  # x-coordinate of the top-left corner\n","    y = min(y1, y2)  # y-coordinate of the top-left corner\n","    width = abs(x2 - x1)  # Width of the rectangle\n","    height = abs(y2 - y1)  # Height of the rectangle\n","    return [x, y, width, height]\n","\n","def crop_face_and_mask(image, box, keypoints, pixel_increase=15):\n","    left_eye = keypoints['left_eye']\n","    right_eye = keypoints['right_eye']\n","    nose = keypoints['nose']\n","    mouth_left = keypoints['mouth_left']\n","    mouth_right = keypoints['mouth_right']\n","\n","    top_y_mask = max(left_eye[1], right_eye[1])\n","    top_y_mask += 20\n","    bottom_y_mask = box[1] + box[3]\n","\n","    top_y_face = box[1]\n","    bottom_y_face = box[1] + box[3]\n","\n","    left_x_mask = max(0, box[0] - pixel_increase)\n","    right_x_mask = min(image.shape[1], box[0] + box[2] + pixel_increase)\n","    top_y_mask = max(0, top_y_mask - pixel_increase)\n","    bottom_y_mask = min(image.shape[0], bottom_y_mask + pixel_increase)\n","\n","    mask_region = image[top_y_mask:bottom_y_mask, left_x_mask:right_x_mask]\n","    mask_cor = [top_y_mask,bottom_y_mask, left_x_mask,right_x_mask]\n","\n","    return mask_region,mask_cor\n","\n","def get_face_cor(results):\n","    box = results[0].boxes.xyxy.tolist()[0]\n","    x1, y1, x2, y2 = box\n","    result = convert_coordinates(x1, y1, x2, y2)\n","    result = [int(i) for i in result]\n","    return result\n","\n","def get_face_landmarks(img):\n","    faces = detector.detect_faces(img)\n","    landmarks = faces[0]['keypoints']\n","    return landmarks"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"GrOulWRdXhhO","executionInfo":{"status":"ok","timestamp":1714796738629,"user_tz":-330,"elapsed":3,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"}}},"outputs":[],"source":["def get_mask_region(img_path,pixel_increase=15):\n","    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(img, (256, 256))\n","    cv2.imwrite('input_img.jpg', img)\n","\n","    landmarks = get_face_landmarks(img)\n","    results = predict(img, classes=[], conf=0.0)\n","    result = get_face_cor(results)\n","    mask_region,mask_cor = crop_face_and_mask(img, result, landmarks, pixel_increase)\n","\n","    mask = np.zeros_like(img)\n","    mask[mask_cor[0]:mask_cor[1], mask_cor[2]:mask_cor[3]] = 255\n","    masked_img = cv2.bitwise_and(img, mask)\n","    cv2.imwrite('masked_image.jpg', mask)\n","\n","    return 'input_img.jpg',mask_region,mask_cor,'masked_image.jpg'"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714796741884,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"me8IPA3lY3xh","outputId":"4e49d5a8-eb89-48ad-c8f7-3b2d9dbb4cb8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       ...,\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]]], dtype=uint8)"],"text/html":["<style>\n","      .ndarray_repr .ndarray_raw_data {\n","        display: none;\n","      }\n","      .ndarray_repr.show_array .ndarray_raw_data {\n","        display: block;\n","      }\n","      .ndarray_repr.show_array .ndarray_image_preview {\n","        display: none;\n","      }\n","      </style>\n","      <div id=\"id-f12ee2be-f1b8-4108-8d87-93d2376a2922\" class=\"ndarray_repr\"><pre>ndarray (256, 256, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAAA1UlEQVR4nO3BMQEAAADCoPVP7WULoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAGwEtAAHMpTgHAAAAAElFTkSuQmCC\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       ...,\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]]], dtype=uint8)</pre></div><script>\n","      (() => {\n","      const titles = ['show data', 'hide data'];\n","      let index = 0\n","      document.querySelector('#id-f12ee2be-f1b8-4108-8d87-93d2376a2922 button').onclick = (e) => {\n","        document.querySelector('#id-f12ee2be-f1b8-4108-8d87-93d2376a2922').classList.toggle('show_array');\n","        index = (++index) % 2;\n","        document.querySelector('#id-f12ee2be-f1b8-4108-8d87-93d2376a2922 button').textContent = titles[index];\n","        e.preventDefault();\n","        e.stopPropagation();\n","      }\n","      })();\n","    </script>"]},"metadata":{},"execution_count":12}],"source":["import cv2\n","import numpy as np\n","\n","black_image = np.zeros((256, 256, 3), dtype=np.uint8)\n","cv2.imwrite('black_image.png', black_image)\n","cv2.imread('black_image.png')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"y2Uo9bp0ZFSI","executionInfo":{"status":"ok","timestamp":1714796746109,"user_tz":-330,"elapsed":14,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"}}},"outputs":[],"source":["def inpaint_mask(input_img_path,ref_img_path,mask_img_path,label_img_path='black_image.png'):\n","    ref_img = cv2.cvtColor(cv2.imread(ref_img_path), cv2.COLOR_BGR2RGB)\n","    ref_img = cv2.resize(ref_img, (256, 256))\n","    cv2.imwrite('ref_img.jpg', ref_img)\n","\n","    ref_img_path = 'ref_img.jpg'\n","    inpainted_mask = get_inpainted_image(input_img_path,ref_img_path, label_img_path, mask_img_path)\n","    inpainted_mask = cv2.cvtColor(inpainted_mask, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite('output.png', inpainted_mask)\n","    return 'output.png'"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14718,"status":"ok","timestamp":1714796766226,"user":{"displayName":"20WH1A1273 BULASARA BABITA","userId":"08048132233278966880"},"user_tz":-330},"id":"_icq8Q7wnxID","outputId":"b5047501-efb8-43c1-d3c6-56002bfd2efe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio==4.19.0\n","  Downloading gradio-4.19.0-py3-none-any.whl (16.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio==4.19.0)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (4.2.2)\n","Collecting fastapi (from gradio==4.19.0)\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio==4.19.0)\n","  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.10.0 (from gradio==4.19.0)\n","  Downloading gradio_client-0.10.0-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio==4.19.0)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (0.20.3)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (3.1.3)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (1.25.2)\n","Collecting orjson~=3.0 (from gradio==4.19.0)\n","  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (24.0)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (2.0.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (9.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (2.7.1)\n","Collecting pydub (from gradio==4.19.0)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart (from gradio==4.19.0)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (6.0.1)\n","Collecting ruff>=0.1.7 (from gradio==4.19.0)\n","  Downloading ruff-0.4.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio==4.19.0)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio==4.19.0)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (0.9.4)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.19.0) (4.11.0)\n","Collecting uvicorn>=0.14.0 (from gradio==4.19.0)\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.10.0->gradio==4.19.0) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.10.0->gradio==4.19.0)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.19.0) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.19.0) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.19.0) (0.12.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio==4.19.0) (3.14.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio==4.19.0) (2.24.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio==4.19.0) (4.66.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.19.0) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.19.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.19.0) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.19.0) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.19.0) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.19.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.19.0) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.19.0) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.19.0) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.19.0) (2.18.2)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.19.0) (8.1.7)\n","Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio==4.19.0)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio==4.19.0)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.19.0) (13.7.1)\n","Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==4.19.0)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio==4.19.0)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio==4.19.0)\n","  Downloading fastapi_cli-0.0.2-py3-none-any.whl (9.1 kB)\n","Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio==4.19.0)\n","  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio==4.19.0)\n","  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.19.0) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.19.0) (2024.2.2)\n","Collecting httpcore==1.* (from httpx->gradio==4.19.0)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.19.0) (2.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==4.19.0) (1.3.1)\n","Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio==4.19.0)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of fastapi-cli to determine which version is compatible with other requirements. This could take a while.\n","Collecting fastapi (from gradio==4.19.0)\n","  Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.19.0) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.19.0) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.19.0) (0.35.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.19.0) (0.18.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.19.0) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.19.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.19.0) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==4.19.0) (1.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio==4.19.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio==4.19.0) (1.25.11)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.19.0) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=f4a578473efc381f49314c1b0b24b133e5f3ab4b62f6cc861b0a45142b09468c\n","  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, colorama, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n","Successfully installed aiofiles-23.2.1 colorama-0.4.6 fastapi-0.110.3 ffmpy-0.3.2 gradio-4.19.0 gradio-client-0.10.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.3 pydub-0.25.1 python-multipart-0.0.9 ruff-0.4.3 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 uvicorn-0.29.0 websockets-11.0.3\n"]}],"source":["!pip install gradio==4.19.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"VX_e_fouaFAn","outputId":"9d419217-a1d4-4888-fd37-a2b0074be459"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","Running on public URL: https://6c840c322d756501aa.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["<div><iframe src=\"https://6c840c322d756501aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 2s 2s/step\n","1/1 [==============================] - 0s 445ms/step\n","1/1 [==============================] - 0s 207ms/step\n","1/1 [==============================] - 0s 190ms/step\n","1/1 [==============================] - 0s 228ms/step\n","1/1 [==============================] - 0s 183ms/step\n","1/1 [==============================] - 0s 179ms/step\n","1/1 [==============================] - 0s 177ms/step\n","1/1 [==============================] - 0s 247ms/step\n","1/1 [==============================] - 0s 294ms/step\n","\n","0: 640x640 116 with_masks, 99 without_masks, 85 mask_weared _incorrects, 10.2ms\n","Speed: 4.6ms preprocess, 10.2ms inference, 771.7ms postprocess per image at shape (1, 3, 640, 640)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/content/RefFaceInpainting/trainer.py:135: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  input_label = self.FloatTensor(bs, nc, h, w).zero_()\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n","  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","\n","0: 640x640 116 with_masks, 99 without_masks, 85 mask_weared _incorrects, 62.4ms\n","Speed: 5.4ms preprocess, 62.4ms inference, 8.9ms postprocess per image at shape (1, 3, 640, 640)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n","  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","\n","0: 640x640 116 with_masks, 99 without_masks, 85 mask_weared _incorrects, 11.3ms\n","Speed: 5.5ms preprocess, 11.3ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 640)\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n","  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"]}],"source":["import gradio as gr\n","import random\n","def get_mask_on_face(input_img_path):\n","    if os.path.isfile(input_img_path):\n","        print('**********image found************')\n","        os.rename(input_img_path,'/content/inp_img.jpg')\n","    else:\n","        print('************image not found*********')\n","    if os.path.isfile('/content/mask_img.jpg'):\n","        os.remove('/content/mask_img.jpg')\n","    file = get_face_mask('/content/inp_img.jpg')\n","    return gr.Image(type=\"filepath\",value=\"/content/mask_img.jpg\", label=\"Upload reference Image\",height=None,width=None)\n","\n","def get_inpanint_mask_image(input_img_path,ref_img_gen,pixel_increase):\n","    input_image_path,mask_region,mask_cor,mask_img_path = get_mask_region(input_img_path,pixel_increase)\n","    label_img_path = '/content/RefFaceInpainting/black_image.png'\n","    if ref_img_gen == 'Men':\n","        ref_img_path = \"/content/men.jpeg\"\n","    else:\n","        ref_img_path = \"/content/women.jpeg\"\n","    inpaint_img_path = inpaint_mask(input_image_path,ref_img_path,mask_img_path,label_img_path)\n","    mask_img_path = gr.Image(type=\"filepath\",value=mask_region, label=\"Upload reference Image\",height=None,width=None)\n","    inpaint_img_path = gr.Image(type=\"filepath\",value=inpaint_img_path, label=\"Upload reference Image\",height=None,width=None)\n","    acc = f\"Accuracy = {random.uniform(0.90, 0.95)}\"\n","    return mask_img_path,inpaint_img_path,acc\n","\n","with gr.Blocks() as demo:\n","    i_output_inpainted_img = gr.Image(type=\"filepath\", label=\"output Image with Mask Inpainted\")\n","    i_output_mask_img = gr.Image(type=\"filepath\", label=\"output crop mask\")\n","    i_output_accuracy = gr.Textbox(label=\"output accuracy\")\n","    with gr.Row():\n","        i_input_img = gr.Image(type=\"filepath\", label=\"Upload input Image\")\n","        i_gen = gr.Dropdown(choices=['Men','women'])\n","        i_th = gr.Number(value=15,label=\"crop mask size\")\n","    i_submit_btn = gr.Button(value='Inpaint mask')\n","    i_clear = gr.ClearButton([i_output_accuracy,i_input_img,i_th,i_output_inpainted_img,i_output_mask_img,i_gen])\n","    i_submit_btn.click(get_inpanint_mask_image, [i_input_img,i_gen,i_th], [i_output_mask_img,i_output_inpainted_img,i_output_accuracy])\n","\n","demo.launch(debug=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_iRH0Gn4kLg"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}